import streamlit as st
import io
from reportlab.pdfgen import canvas
from .database import Database
from .pdf_processor import PDFProcessor
from .vector_store import VectorStore
from .web_search import WebSearch
from .compare import PaperComparator
from .nlp import NLPProcessor
import fitz
import os
import re
import uuid

def render_agent_ui(db: Database, nlp: NLPProcessor, web_search: WebSearch, comparator: PaperComparator, vector_store: VectorStore):
    st.header("ğŸ§  è‡ªç„¶èªè¨€æŒ‡ä»¤ (Agent æ¨¡å¼)")
    user_command = st.text_input(
        "è«‹è¼¸å…¥æŒ‡ä»¤ï¼š",
        placeholder="ä¾‹å¦‚ï¼šæŸ¥è©¢ arxiv çš„ vitã€æ¯”è¼ƒ arxiv ç¬¬äºŒç¯‡èˆ‡æœ¬åœ°å…­ç¯‡"
    )

    if user_command:
        if len(user_command) > 500:
            st.error("âŒ æŒ‡ä»¤éé•·ï¼Œè«‹ç¸®çŸ­è‡³500å­—ä»¥å…§")
            return
        with st.spinner("AI ç†è§£ä¸­..."):
            intent, params = nlp.parse_user_intent(user_command)
            
            # Ensure papers directory exists
            os.makedirs("papers", exist_ok=True)
            
            # Determine search source and keyword(s) for display
            source_map = {
                "/history": "æœ¬åœ°è³‡æ–™åº«",
                "local_query": "æœ¬åœ°è³‡æ–™åº«",
                "arxiv_search": "arXiv",
                "semantic_search": "Semantic Scholar",
                "compare_custom": "æœ¬åœ°è³‡æ–™åº« (æ¯”è¼ƒ)",
                "compare": "æœ¬åœ°è³‡æ–™åº« (æ¯”è¼ƒ)",
                "arxiv_vs_local_compare": "arXiv + æœ¬åœ°è³‡æ–™åº« (æ¯”è¼ƒ)",
                "compare_web_results": f"{st.session_state['last_web_search']['type'].capitalize() if 'last_web_search' in st.session_state else 'Web'} (æ¯”è¼ƒ)",
                "compare_arxiv_local": "arXiv + æœ¬åœ°è³‡æ–™åº« (æ¯”è¼ƒ)",
                "unknown": "æœªçŸ¥"
            }
            search_source = source_map.get(intent, "æœªçŸ¥")
            keywords = "ç„¡"
            exclude_words = {'arxiv', 'semantic', 'scholar', 'query', 'search'}
            if intent == "arxiv_search":
                keywords = ' '.join(w for w in params.split() if w.lower() not in exclude_words)
            elif intent == "semantic_search":
                keyword, _, _ = params
                keywords = ' '.join(w for w in keyword.split() if w.lower() not in exclude_words)
            elif intent == "local_query":
                keyword, _ = params
                keywords = ' '.join(w for w in keyword.split() if w.lower() not in exclude_words)
            elif intent == "arxiv_vs_local_compare":
                keyword, _ = params
                keywords = ' '.join(w for w in keyword.split() if w.lower() not in exclude_words)
            elif intent == "compare_custom":
                indices, topic = params
                keywords = topic if topic else "ç„¡"
            elif intent == "compare":
                keywords = params if params else "ç„¡"
            elif intent in ["compare_web_results", "compare_arxiv_local"]:
                indices, topic = params
                keywords = topic if topic else st.session_state.get('last_search_keyword', "ç„¡")
                keywords = ' '.join(w for w in keywords.split() if w.lower() not in exclude_words)
            
            # Display search source and keywords
            st.markdown(f"**æœå°‹ä¾†æº**: {search_source}")
            st.markdown(f"**é—œéµè©**: {keywords}")

            # Process the command
            if intent == "/history":
                papers = db.get_papers()
                if papers:
                    st.markdown("### ğŸ—‚ï¸ æœ¬åœ°è«–æ–‡æ¸…å–®ï¼š")
                    for idx, (pid, title, _) in enumerate(papers, 1):
                        st.markdown(f"{idx}. [{title}] (è³‡æ–™åº«ID: {pid})")
                else:
                    st.warning("æœ¬åœ°è³‡æ–™åº«ç„¡è«–æ–‡ã€‚")
            elif intent == "local_query":
                keyword, query_embedding = params
                st.markdown("### ğŸ” æœ¬åœ°æ‘˜è¦æŸ¥è©¢çµæœï¼š")
                
                # Database metadata search
                papers = db.get_papers()
                metadata_results = []
                for pid, title, abstract in papers:
                    if keyword and (re.search(r'\b' + re.escape(keyword) + r'\b', title, re.IGNORECASE) or 
                                    re.search(r'\b' + re.escape(keyword) + r'\b', abstract, re.IGNORECASE)):
                        metadata_results.append({"pid": pid, "title": title, "abstract": abstract})
                
                # Vector search for PDFs
                pdf_results = []
                pdf_files = [f"papers/{f}" for f in os.listdir("papers") if f.endswith(".pdf")]
                if pdf_files and query_embedding:
                    documents = vector_store.create_documents(pdf_files)
                    pdf_results = vector_store.semantic_search(query_embedding, documents, top_k=5)
                
                # Combine and deduplicate results
                seen_titles = set()
                combined_results = []
                for res in metadata_results:
                    if res["title"] not in seen_titles:
                        combined_results.append(res)
                        seen_titles.add(res["title"])
                for res in pdf_results:
                    file_name = os.path.basename(res["file_path"])
                    title = file_name.replace(".pdf", "")  # Fallback title
                    abstract = res["text"][:500] + "..."
                    for pid, db_title, db_abstract in papers:
                        if db_title in res["text"] or file_name in db_title:
                            title = db_title
                            abstract = db_abstract
                            break
                    if title not in seen_titles:
                        combined_results.append({"pid": None, "title": title, "abstract": abstract, "file_path": res["file_path"]})
                        seen_titles.add(title)
                
                # Display results
                if combined_results:
                    for i, res in enumerate(combined_results, 1):
                        st.markdown(f"**{i}. {res['title']}**")
                        st.markdown(f"> {res['abstract'][:500]}...")
                        if st.button(f"â• åŒ¯å…¥ç¬¬ {i} ç­† (æœ¬åœ°)", key=f"local_import_{i}_{uuid.uuid4()}"):
                            file_hash = PDFProcessor().get_file_hash((res["title"] + res["abstract"]).encode("utf-8"))
                            if db.insert_metadata(res["title"], res["abstract"], file_hash, "local_query"):
                                st.success(f"âœ… å·²åŒ¯å…¥ï¼š{res['title'][:50]}")
                            else:
                                st.warning(f"âš ï¸ å·²å­˜åœ¨æˆ–åŒ¯å…¥å¤±æ•—")
                else:
                    st.warning("æœªæ‰¾åˆ°ç›¸é—œè«–æ–‡ã€‚")
            elif intent == "arxiv_search":
                session_key = f"arxiv_results_{uuid.uuid4()}"
                results = web_search.search_arxiv(params)
                st.session_state[session_key] = results
                st.session_state['last_web_search'] = {'type': 'arxiv', 'key': session_key}
                if results:
                    for i, (title, abstract, link) in enumerate(results, 1):
                        st.markdown(f"**{i}. [{title}]({link})**")
                        st.markdown(f"> {abstract}")
                        if st.button(f"â• åŒ¯å…¥ç¬¬ {i} ç­†", key=f"agent_import_arxiv_{i}_{uuid.uuid4()}"):
                            file_hash = PDFProcessor().get_file_hash((title + abstract).encode("utf-8"))
                            if db.insert_metadata(title, abstract, file_hash, "web_search"):
                                st.success(f"âœ… å·²åŒ¯å…¥ç¬¬ {i} ç­†æ‘˜è¦ï¼š{title[:50]}")
                            else:
                                st.warning(f"âš ï¸ ç¬¬ {i} ç­†å·²å­˜åœ¨æˆ–åŒ¯å…¥å¤±æ•—")
                else:
                    st.warning("arXiv æœå°‹ç„¡çµæœã€‚")
            elif intent == "semantic_search":
                session_key = f"semantic_results_{uuid.uuid4()}"
                keyword, max_results, days = params
                results = web_search.search_semantic_scholar(keyword, max_results=max_results, days=days)
                st.session_state[session_key] = results
                st.session_state['last_web_search'] = {'type': 'semantic', 'key': session_key}
                if results:
                    for i, paper in enumerate(results, 1):
                        st.markdown(f"**{i}. [{paper['title']}]({paper['url']})**")
                        st.markdown(f"ä½œè€…: {paper['authors']}")
                        st.markdown(f"å¹´ä»½: {paper['year']}")
                        st.markdown(f"æœŸåˆŠ/æœƒè­°: {paper['venue']}")
                        st.markdown(f"å¼•ç”¨æ•¸: {paper['citation_count']}")
                        st.markdown(f"> {paper['abstract']}")
                        if paper['doi']:
                            st.markdown(f"DOI: {paper['doi']}")
                        if st.button(f"â• åŒ¯å…¥ç¬¬ {i} ç­† (Semantic)", key=f"semantic_import_{i}_{uuid.uuid4()}"):
                            file_hash = PDFProcessor().get_file_hash((paper['title'] + paper['abstract']).encode("utf-8"))
                            if db.insert_metadata(paper['title'], paper['abstract'], file_hash, "Semantic Scholar"):
                                st.success(f"âœ… å·²åŒ¯å…¥ç¬¬ {i} ç­†æ‘˜è¦ï¼š{paper['title'][:50]}")
                            else:
                                st.warning(f"âš ï¸ ç¬¬ {i} ç­†å·²å­˜åœ¨æˆ–åŒ¯å…¥å¤±æ•—")
                else:
                    st.warning("Semantic Scholar æœå°‹ç„¡çµæœã€‚")
            elif intent == "compare_custom":
                indices, topic = params
                papers = db.get_papers()
                try:
                    title1, abs1 = papers[indices[0] - 1][1:3]
                    title2, abs2 = papers[indices[1] - 1][1:3]
                    st.markdown(f"#### ğŸ“˜ æ¯”è¼ƒå°è±¡ 1ï¼ˆç¬¬{indices[0]}ç¯‡ï¼‰ï¼š{title1}")
                    st.markdown(f"> {abs1[:500]}...")
                    st.markdown(f"#### ğŸ“™ æ¯”è¼ƒå°è±¡ 2ï¼ˆç¬¬{indices[1]}ç¯‡ï¼‰ï¼š{title2}")
                    st.markdown(f"> {abs2[:500]}...")
                    result = comparator.compare_abstracts(abs1, abs2, topic)
                    st.markdown("### ğŸ“‹ æ¯”è¼ƒçµæœï¼š")
                    st.markdown(result)
                except IndexError:
                    st.error("âŒ é¸æ“‡çš„ç¯‡æ•¸è¶…å‡ºç¯„åœã€‚")
            elif intent == "compare":
                topic = params
                papers = db.get_papers()
                if len(papers) >= 2:
                    title1, abs1 = papers[0][1:3]
                    title2, abs2 = papers[1][1:3]
                    st.markdown(f"#### ğŸ“˜ æ¯”è¼ƒå°è±¡ 1ï¼ˆç¬¬1ç¯‡ï¼‰ï¼š{title1}")
                    st.markdown(f"> {abs1[:500]}...")
                    st.markdown(f"#### ğŸ“™ æ¯”è¼ƒå°è±¡ 2ï¼ˆç¬¬2ç¯‡ï¼‰ï¼š{title2}")
                    st.markdown(f"> {abs2[:500]}...")
                    result = comparator.compare_abstracts(abs1, abs2, topic)
                    st.markdown("### ğŸ“‹ æ¯”è¼ƒçµæœï¼š")
                    st.markdown(result)
                else:
                    st.error("âŒ è³‡æ–™åº«ä¸­éœ€è‡³å°‘æœ‰å…©ç¯‡è«–æ–‡æ‰èƒ½æ¯”è¼ƒã€‚")
            elif intent == "arxiv_vs_local_compare":
                keyword, local_index = params
                papers = db.get_papers()
                try:
                    local_title, local_abs = papers[local_index - 1][1:3]
                    arxiv_results = web_search.search_arxiv(keyword, max_results=1)
                    if arxiv_results:
                        arxiv_title, arxiv_abs, _ = arxiv_results[0]
                        st.markdown(f"#### ğŸ“˜ æœ¬åœ°è«–æ–‡ï¼ˆç¬¬{local_index}ç¯‡ï¼‰ï¼š{local_title}")
                        st.markdown(f"> {local_abs[:500]}...")
                        st.markdown(f"#### ğŸ“™ arXiv è«–æ–‡ï¼š{arxiv_title}")
                        st.markdown(f"> {arxiv_abs[:500]}...")
                        result = comparator.compare_abstracts(local_abs, arxiv_abs, keyword)
                        st.markdown("### ğŸ“‹ æ¯”è¼ƒçµæœï¼š")
                        st.markdown(result)
                    else:
                        st.warning("arXiv æœå°‹ç„¡çµæœï¼Œç„¡æ³•æ¯”è¼ƒã€‚")
                except IndexError:
                    st.error("âŒ æœ¬åœ°è«–æ–‡ç·¨è™Ÿè¶…å‡ºç¯„åœã€‚")
            elif intent == "compare_web_results":
                indices, topic = params
                if 'last_web_search' not in st.session_state:
                    st.error("âŒ ç„¡è¿‘æœŸ Web æœç´¢çµæœï¼Œè«‹å…ˆåŸ·è¡Œ arXiv æˆ– Semantic Scholar æœç´¢ã€‚")
                    return
                search_type = st.session_state['last_web_search']['type']
                session_key = st.session_state['last_web_search']['key']
                results = st.session_state.get(session_key, [])
                
                try:
                    if not results:
                        st.error("âŒ å…ˆå‰æœç´¢çµæœç‚ºç©ºï¼Œè«‹é‡æ–°åŸ·è¡Œæœç´¢ã€‚")
                        return
                    if max(indices) > len(results):
                        st.error(f"âŒ ç´¢å¼•è¶…å‡ºç¯„åœï¼ˆåƒ…æœ‰ {len(results)} ç¯‡è«–æ–‡ï¼‰ã€‚")
                        return
                    if search_type == 'arxiv':
                        title1, abs1, _ = results[indices[0] - 1]
                        title2, abs2, _ = results[indices[1] - 1]
                    else:  # semantic
                        title1, abs1 = results[indices[0] - 1]['title'], results[indices[0] - 1]['abstract']
                        title2, abs2 = results[indices[1] - 1]['title'], results[indices[1] - 1]['abstract']
                    
                    st.markdown(f"#### ğŸ“˜ æ¯”è¼ƒå°è±¡ 1ï¼ˆç¬¬{indices[0]}ç¯‡ï¼‰ï¼š{title1}")
                    st.markdown(f"> {abs1[:500] if abs1 else '(ç„¡æ‘˜è¦)'}...")
                    st.markdown(f"#### ğŸ“™ æ¯”è¼ƒå°è±¡ 2ï¼ˆç¬¬{indices[1]}ç¯‡ï¼‰ï¼š{title2}")
                    st.markdown(f"> {abs2[:500] if abs2 else '(ç„¡æ‘˜è¦)'}...")
                    if not abs1 or not abs2 or "(No abstract)" in [abs1, abs2]:
                        st.warning("âš ï¸ ä¸€ç¯‡æˆ–å¤šç¯‡è«–æ–‡ç¼ºå°‘æœ‰æ•ˆæ‘˜è¦ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒã€‚")
                        return
                    result = comparator.compare_abstracts(abs1, abs2, topic)
                    st.markdown("### ğŸ“‹ æ¯”è¼ƒçµæœï¼š")
                    st.markdown(result)
                except IndexError:
                    st.error("âŒ é¸æ“‡çš„ç¯‡æ•¸è¶…å‡ºç¯„åœã€‚")
            elif intent == "compare_arxiv_local":
                indices, topic = params
                arxiv_index, local_index = indices
                if 'last_web_search' not in st.session_state:
                    st.error("âŒ ç„¡è¿‘æœŸ arXiv æœç´¢çµæœï¼Œè«‹å…ˆåŸ·è¡Œ arXiv æœç´¢ã€‚")
                    return
                search_type = st.session_state['last_web_search']['type']
                if search_type != 'arxiv':
                    st.error("âŒ å…ˆå‰æœç´¢é arXiv ä¾†æºï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒã€‚")
                    return
                session_key = st.session_state['last_web_search']['key']
                arxiv_results = st.session_state.get(session_key, [])
                local_papers = db.get_papers()
                
                try:
                    if not arxiv_results:
                        st.error("âŒ arXiv æœç´¢çµæœç‚ºç©ºï¼Œè«‹é‡æ–°åŸ·è¡Œæœç´¢ã€‚")
                        return
                    if arxiv_index > len(arxiv_results):
                        st.error(f"âŒ arXiv ç´¢å¼•è¶…å‡ºç¯„åœï¼ˆåƒ…æœ‰ {len(arxiv_results)} ç¯‡è«–æ–‡ï¼‰ã€‚")
                        return
                    if not local_papers:
                        st.error("âŒ æœ¬åœ°è³‡æ–™åº«ç„¡è«–æ–‡ï¼Œè«‹å…ˆä¸Šå‚³è«–æ–‡ã€‚")
                        return
                    if local_index > len(local_papers):
                        st.error(f"âŒ æœ¬åœ°ç´¢å¼•è¶…å‡ºç¯„åœï¼ˆåƒ…æœ‰ {len(local_papers)} ç¯‡è«–æ–‡ï¼‰ã€‚")
                        return
                    
                    # Extract arXiv paper
                    arxiv_title, arxiv_abs, _ = arxiv_results[arxiv_index - 1]
                    # Extract local paper
                    local_title, local_abs = local_papers[local_index - 1][1:3]
                    
                    st.markdown(f"#### ğŸ“˜ arXiv è«–æ–‡ï¼ˆç¬¬{arxiv_index}ç¯‡ï¼‰ï¼š{arxiv_title}")
                    st.markdown(f"> {arxiv_abs[:500] if arxiv_abs else '(ç„¡æ‘˜è¦)'}...")
                    st.markdown(f"#### ğŸ“™ æœ¬åœ°è«–æ–‡ï¼ˆç¬¬{local_index}ç¯‡ï¼‰ï¼š{local_title}")
                    st.markdown(f"> {local_abs[:500] if local_abs else '(ç„¡æ‘˜è¦)'}...")
                    
                    if not arxiv_abs or not local_abs or "(No abstract)" in [arxiv_abs, local_abs]:
                        st.warning("âš ï¸ ä¸€ç¯‡æˆ–å¤šç¯‡è«–æ–‡ç¼ºå°‘æœ‰æ•ˆæ‘˜è¦ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒã€‚")
                        return
                    result = comparator.compare_abstracts(arxiv_abs, local_abs, topic)
                    st.markdown("### ğŸ“‹ æ¯”è¼ƒçµæœï¼š")
                    st.markdown(result)
                except IndexError:
                    st.error("âŒ é¸æ“‡çš„ç¯‡æ•¸è¶…å‡ºç¯„åœã€‚")
            else:
                st.error(f"âŒ æœªçŸ¥æŒ‡ä»¤ï¼š{user_command}ã€‚è«‹å˜—è©¦ä¾‹å¦‚ï¼šæŸ¥è©¢ arxiv çš„ vitã€æ¯”è¼ƒ arxiv ç¬¬äºŒç¯‡èˆ‡æœ¬åœ°å…­ç¯‡")

def render_upload_ui(db: Database, processor: PDFProcessor):
    st.sidebar.header("\U0001F4E5 ä¸Šå‚³ PDF")
    uploaded_files = st.sidebar.file_uploader("é¸æ“‡ PDF ä¸Šå‚³ï¼š", type="pdf", accept_multiple_files=True)
    if uploaded_files:
        known_hashes = db.get_known_hashes()
        for uploaded_file in uploaded_files:
            file_bytes = uploaded_file.read()
            file_hash = processor.get_file_hash(file_bytes)
            if file_hash in known_hashes:
                st.sidebar.warning(f"âš ï¸ æª”æ¡ˆå·²å­˜åœ¨ï¼š{uploaded_file.name}")
                continue
            title, abstract, _ = processor.extract_title_abstract(file_bytes)
            db.insert_metadata(title, abstract, file_hash, "web_upload")
            st.sidebar.success(f"âœ… å·²ä¸Šå‚³ï¼š{title[:40]}...")
            with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                pix = doc.load_page(0).get_pixmap(matrix=fitz.Matrix(2, 2))
                st.image(pix.tobytes("png"), caption=f"PDF é è¦½ - {uploaded_file.name}", use_column_width=True)
        st.rerun()

def render_download_ui(db: Database):
    st.sidebar.header("ğŸ“¤ ä¸‹è¼‰æ‘˜è¦ PDF")
    papers = db.get_papers()
    options = {f"[{pid}] {title[:40]}...": (pid, title, abstract) for pid, title, abstract in papers}
    selected = st.sidebar.selectbox("é¸æ“‡è¦ä¸‹è¼‰çš„æ‘˜è¦ï¼š", list(options.keys()))
    if st.sidebar.button("ğŸ“„ ä¸‹è¼‰æ‘˜è¦ PDF"):
        pid, title, abstract = options[selected]
        buffer = io.BytesIO()
        c = canvas.Canvas(buffer)
        c.setFont("Helvetica-Bold", 14)
        c.drawString(40, 800, f"Title: {title[:80]}")
        c.setFont("Helvetica", 12)
        y = 770
        for line in abstract.split('\n'):
            for wrap_line in [line[i:i+90] for i in range(0, len(line), 90)]:
                if y < 40:
                    c.showPage()
                    y = 800
                    c.setFont("Helvetica", 12)
                c.drawString(40, y, wrap_line)
                y -= 18
        c.save()
        buffer.seek(0)
        st.sidebar.download_button(
            label=f"ğŸ“¥ ä¸‹è¼‰ [{pid}] {title[:20]}...",
            data=buffer,
            file_name=f"abstract_{pid}.pdf",
            mime="application/pdf"
        )